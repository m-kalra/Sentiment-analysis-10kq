{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESLA 10-K and Q FILLING SENTIMENT ANALYSIS\n",
    "\n",
    "### - MANAVV KALRA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict=pd.read_csv('resources/LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "\n",
    "with open('resources/Stopwords.txt' ,'r') as file:\n",
    "    stopwords = file.read().lower()\n",
    "stopwords_ls = stopwords.split('\\n')\n",
    "\n",
    "positive_words=list(master_dict['Word'].loc[master_dict['Positive']!=0])\n",
    "positive_words=[x.lower() for x in positive_words]\n",
    "negative_words=list(master_dict['Word'].loc[master_dict['Negative']!=0])\n",
    "negative_words=[x.lower() for x in negative_words]\n",
    "uncertainty_words=list(master_dict['Word'].loc[master_dict['Uncertainty']!=0])\n",
    "uncertainty_words=[x.lower() for x in uncertainty_words]\n",
    "constraining_words=list(master_dict['Word'].loc[master_dict['Constraining']!=0])\n",
    "constraining_words=[x.lower() for x in constraining_words]\n",
    "litigious_words=list(master_dict['Word'].loc[master_dict['Litigious']!=0])\n",
    "litigious_words=[x.lower() for x in litigious_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-K/A</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/131860...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/131860...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/131860...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2020-10-26</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/131860...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/131860...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description Filing date  \\\n",
       "0    10-K/A  Annual report [Section 13 and 15(d), not S-K I...  2021-04-30   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]  2021-04-28   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...  2021-02-08   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]  2020-10-26   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]  2020-07-28   \n",
       "\n",
       "  Reporting date                                        Filings URL  \n",
       "0     2020-12-31  https://www.sec.gov/Archives/edgar/data/131860...  \n",
       "1     2021-03-31  https://www.sec.gov/Archives/edgar/data/131860...  \n",
       "2     2020-12-31  https://www.sec.gov/Archives/edgar/data/131860...  \n",
       "3     2020-09-30  https://www.sec.gov/Archives/edgar/data/131860...  \n",
       "4     2020-06-30  https://www.sec.gov/Archives/edgar/data/131860...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('resources/tesla.xlsx')\n",
    "df=df.iloc[:28,:] #LAST FIVE YEARS\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_url(txt):\n",
    "    txt=txt.replace('-index.htm','.txt')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Filings URL']=df['Filings URL'].map(replace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 3 lists\n",
    "mda_ls=[]\n",
    "qqdmr_ls=[]\n",
    "rf_ls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular expressions\n",
    "\n",
    "#HTML tags\n",
    "html= r\"<[^>]*>\"\n",
    "\n",
    "#Management's Discussion and Analysis\n",
    "mda = r\"item\\s*\\d\\.\\s*Management\\'s Discussion and Analysis.*?item\\s*\\d\\(?[a-z]?\\)?\\.\\s*\"\n",
    "\n",
    "#Quantitavite and Qualitative Disclosures about Market Risk\n",
    "qqdmr = r\"item\\s*\\d\\(?[a-z]?\\)?\\.\\s*Quantitative and Qualitative Disclosures about Market Risk.*?item\\s*\\d\\(?[a-z]?\\)?\\.\\s*\"\n",
    "    \n",
    "#Risk Factors\n",
    "rf = r\"item\\s*\\d\\(?[a-z]?\\)?\\.\\s*Risk Factors.*?item\\s*\\d\\(?[a-z]?\\)?\\.\\s*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean(txt) and prep(url) functions\n",
    "\n",
    "def clean(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('&nbsp;', ' ').replace('\\xa0',' ')\n",
    "    return txt\n",
    "\n",
    "def prep(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    txt = str(soup)\n",
    "    txt = re.sub(html,' ',txt) #remove HTML tags\n",
    "    all_ls=[[mda,mda_ls],[qqdmr,qqdmr_ls],[rf,rf_ls]]\n",
    "    for regex,ls in all_ls:\n",
    "        check=re.findall(regex, txt, re.I | re.M | re.DOTALL)\n",
    "        if check:\n",
    "            ls.append(clean(max(check, key=len)))\n",
    "        else:\n",
    "            ls.append('')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "20    None\n",
       "21    None\n",
       "22    None\n",
       "23    None\n",
       "24    None\n",
       "25    None\n",
       "26    None\n",
       "27    None\n",
       "Name: Filings URL, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Filings URL'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For tokenizing (Words and sentences)\n",
    "from nltk.tokenize import RegexpTokenizer,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE STOP WORDS\n",
    "def remove_stopwords(tokens):\n",
    "    return list(filter(lambda x: x not in stopwords_ls, tokens))\n",
    "\n",
    "#CONVERT THE TEXT INTO TOKENS\n",
    "def tokenize(txt):\n",
    "    txt = txt.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return remove_stopwords(tokens)\n",
    "\n",
    "#CALCULATE POSITIVE WORD SCORE\n",
    "def calc_positive_score(txt):\n",
    "    n_positive=0\n",
    "    tokens=tokenize(txt)\n",
    "    for token in tokens:\n",
    "        if token in positive_words:\n",
    "            n_positive+=1\n",
    "    return n_positive\n",
    "\n",
    "#CALCULATE NEGATIVE WORD SCORE\n",
    "def calc_negative_score(txt):\n",
    "    n_negative=0\n",
    "    tokens=tokenize(txt)\n",
    "    for token in tokens:\n",
    "        if token in negative_words:\n",
    "            n_negative-=1\n",
    "    negative_score=-1*n_negative\n",
    "    return negative_score\n",
    "\n",
    "#CALCULATE POLARITY SCORE\n",
    "def calc_polarity_score(positive_score,negative_score):\n",
    "    polarity_score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "    return polarity_score\n",
    "\n",
    "#CALCULATE AVERAGE SENTENCE LENGTH\n",
    "def calc_avg_sentence_length(txt):\n",
    "    avg_sentence_length=0\n",
    "    sentences = sent_tokenize(txt)\n",
    "    tokens = tokenize(txt)\n",
    "    if len(sentences) != 0:\n",
    "        avg_sentence_length=len(tokens)/len(sentences)\n",
    "    return avg_sentence_length\n",
    "\n",
    "#CALCULATE WORD COUNT\n",
    "def calc_wordcount(txt):\n",
    "    n_words=len(tokenize(txt))\n",
    "    return n_words\n",
    "\n",
    "#CALCULATE COMPLEX WORD COUNT\n",
    "def calc_complexword_count(txt):\n",
    "    tokens = tokenize(txt)\n",
    "    n_complexwords = 0\n",
    "    vowels = 'aeiou'\n",
    "    for token in tokens:\n",
    "        n_syllables = 0\n",
    "        if token.endswith(('es','ed')):\n",
    "            pass\n",
    "        if token[0] in vowels:\n",
    "            n_syllables+=1\n",
    "        for i in range(1,len(token)):\n",
    "            if token[i] in vowels and token[i-1] not in vowels:\n",
    "                n_syllables+=1\n",
    "        if n_syllables==0:\n",
    "            n_syllables+=1\n",
    "        if n_syllables>2:\n",
    "            n_complexwords+=1\n",
    "    return n_complexwords\n",
    "\n",
    "#CALCULATE COMPLEX WORD PERCENTAGE\n",
    "def calc_percentage_complexwords(txt):\n",
    "    percentage_complexwords=0\n",
    "    n_complexwords=calc_complexword_count(txt)\n",
    "    n_words=calc_wordcount(txt)\n",
    "    if n_words!=0:\n",
    "        percentage_complexwords=n_complexwords/n_words\n",
    "    return percentage_complexwords\n",
    "    \n",
    "#CALCULATE FOG INDEX    \n",
    "def calc_fog_index(avg_sentence_length,percentage_complexwords):\n",
    "    fog_index=0.4 * (avg_sentence_length + percentage_complexwords)\n",
    "    return fog_index\n",
    "\n",
    "#CALCULATE UNCERTAINTY SCORE\n",
    "def calc_uncertainty_score(txt):\n",
    "    uncertainty_score=0\n",
    "    tokens=tokenize(txt)\n",
    "    for token in tokens:\n",
    "        if token in uncertainty_words:\n",
    "            uncertainty_score+=1\n",
    "    return uncertainty_score\n",
    "\n",
    "#CALCULATE CONSTRAINING SCORE\n",
    "def calc_constraining_score(txt):\n",
    "    constraining_score=0\n",
    "    tokens=tokenize(txt)\n",
    "    for token in tokens:\n",
    "        if token in constraining_words:\n",
    "            constraining_score+=1\n",
    "    return constraining_score\n",
    "\n",
    "#CALCULATE LITIGIOUS SCORE\n",
    "def calc_litigious_score(txt):\n",
    "    litigious_score=0\n",
    "    tokens=tokenize(txt)\n",
    "    for token in tokens:\n",
    "        if token in litigious_words:\n",
    "            litigious_score+=1\n",
    "    return litigious_score\n",
    "\n",
    "#CALCULATE POSITIVE WORD PROPORTION\n",
    "def calc_positiveword_proportion(positive_score,n_words):\n",
    "    positiveword_proportion=0\n",
    "    if n_words!=0:\n",
    "        positiveword_proportion=positive_score/n_words\n",
    "    return positiveword_proportion\n",
    "\n",
    "#CALCULATE NEGATIVE WORD PROPORTION\n",
    "def calc_negativeword_proportion(negative_score,n_words):\n",
    "    negativeword_proportion=0\n",
    "    if n_words!=0:\n",
    "        negativeword_proportion=negative_score/n_words\n",
    "    return negativeword_proportion\n",
    "\n",
    "#CALCULATE UNCERTAINTY WORD PROPORTION\n",
    "def calc_uncertaintyword_proportion(uncertainty_score,n_words):\n",
    "    uncertaintyword_proportion=0\n",
    "    if n_words!=0:\n",
    "        uncertaintyword_proportion=uncertainty_score/n_words\n",
    "    return uncertaintyword_proportion\n",
    "\n",
    "#CALCULATE CONSTRAINING WORD PROPORTION\n",
    "def calc_constrainingword_proportion(constraining_score,n_words):\n",
    "    constrainingword_proportion=0\n",
    "    if n_words!=0:\n",
    "        constrainingword_proportion=constraining_score/n_words\n",
    "    return constrainingword_proportion\n",
    "\n",
    "#CALCULATE LITIGIOUS WORD PROPORTION\n",
    "def calc_litigiousword_proportion(litigious_score,n_words):\n",
    "    litigiousword_proportion=0\n",
    "    if n_words!=0:\n",
    "        litigiousword_proportion=litigious_score/n_words\n",
    "    return litigiousword_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the functions to all three lists and storing the results in the dataframe\n",
    "\n",
    "apply_ls=[['mda',mda_ls],['qqdmr',qqdmr_ls],['rf',rf_ls]]\n",
    "\n",
    "for section,ls in apply_ls:\n",
    "    df[section+'_positive_score'] = list(map(calc_positive_score,ls))\n",
    "    df[section+'_negative_score'] = list(map(calc_negative_score,ls))\n",
    "    df[section+'_polarity_score'] = np.vectorize(calc_polarity_score)(df[section+'_positive_score'],df[section+'_negative_score'])\n",
    "    df[section+'_average_sentence_length'] = list(map(calc_avg_sentence_length,ls))\n",
    "    df[section+'_percentage_of_complex_words'] = list(map(calc_percentage_complexwords,ls))\n",
    "    df[section+'_fog_index'] = np.vectorize(calc_fog_index)(df[section+'_average_sentence_length'],df[section+'_percentage_of_complex_words'])\n",
    "    df[section+'_complex_word_count'] = list(map(calc_complexword_count,ls))    \n",
    "    df[section+'_word_count'] = list(map(calc_wordcount,ls))\n",
    "    df[section+'_uncertainty_score'] = list(map(calc_uncertainty_score,ls))\n",
    "    df[section+'_constraining_score'] = list(map(calc_constraining_score,ls))\n",
    "    df[section+'_litigious_score'] = list(map(calc_litigious_score,ls))\n",
    "    df[section+'_positive_word_proportion'] = np.vectorize(calc_positiveword_proportion)(df[section+'_positive_score'],df[section+'_word_count'])\n",
    "    df[section+'_negative_word_proportion'] = np.vectorize(calc_negativeword_proportion)(df[section+'_negative_score'],df[section+'_word_count'])\n",
    "    df[section+'_uncertainty_word_proportion'] = np.vectorize(calc_uncertaintyword_proportion)(df[section+'_uncertainty_score'],df[section+'_word_count'])\n",
    "    df[section+'_constraining_word_proportion'] = np.vectorize(calc_constrainingword_proportion)(df[section+'_constraining_score'],df[section+'_word_count'])\n",
    "    df[section+'_litigious_word_proportion'] = np.vectorize(calc_litigiousword_proportion)(df[section+'_litigious_score'],df[section+'_word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('tesla_result.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
